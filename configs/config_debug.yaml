# =============================================================================
# SBLDM Configuration - Quick Debug Run
# Target: Fast iteration on Kaggle T4/P100
# Resolution: 64x64 for speed
# =============================================================================

# Experiment settings
experiment:
  name: "sbldm_debug_64"
  seed: 42
  output_dir: "./outputs_debug"
  log_dir: "./logs_debug"
  checkpoint_dir: "./checkpoints_debug"
  
# Data configuration
data:
  dataset: "brats"
  data_dir: "./data/processed"
  resolution: 64  # Smaller for debug
  channels: 1
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  num_workers: 2
  pin_memory: true
  max_samples: 500  # Limit dataset size for debug
  
# VAE configuration
vae:
  in_channels: 1
  latent_channels: 4
  hidden_dims: [32, 64, 128]  # Smaller network
  downsample_factor: 4  # 64 -> 16 latent
  
  # Training
  batch_size: 32  # Reduced for stability
  learning_rate: 1.0e-4
  weight_decay: 1.0e-5
  epochs: 20  # Quick training
  
  # Loss weights
  kl_weight: 1.0e-6  # Very small KL weight to start
  kl_anneal: true  # Enable annealing
  kl_anneal_epochs: 10  # Slowly increase KL weight
  perceptual_weight: 0.0
  
  # Optimization
  scheduler: "cosine"
  warmup_epochs: 2
  gradient_clip: 1.0
  
# UNet configuration
unet:
  in_channels: 4
  out_channels: 4
  model_channels: 32  # Smaller
  channel_mult: [1, 2, 2]
  num_res_blocks: 1  # Fewer blocks
  attention_resolutions: [4]
  dropout: 0.0
  use_scale_shift_norm: true
  
# Diffusion configuration
diffusion:
  timesteps: 1000
  beta_schedule: "cosine"
  beta_start: 1.0e-4
  beta_end: 0.02
  gamma: 1.0
  
  loss_type: "mse"
  use_freq_loss: false  # Disable for debug
  use_latent_cutmix: false
  
  batch_size: 32
  learning_rate: 2.0e-4
  weight_decay: 0.0
  training_steps: 2000  # Quick run
  
  scheduler: "cosine"
  warmup_steps: 100
  gradient_clip: 1.0
  ema_decay: 0.999
  
  log_interval: 50
  sample_interval: 500
  save_interval: 1000
  
# Sampling configuration
sampling:
  sampler: "ddim"
  ddim_steps: 20  # Fast sampling
  ddim_eta: 0.0
  
  use_adaptive_sampling: false
  
  num_samples: 8
  batch_size: 8
  
# Evaluation configuration
eval:
  compute_fid: false  # Skip for debug
  compute_ssim: true
  compute_psnr: true
  compute_lpips: false
  
  save_samples: true
  save_reconstructions: true
  save_error_heatmaps: false
  num_visualize: 8
  
# Hardware optimization
hardware:
  mixed_precision: true
  channels_last: false
  compile_model: false
  gradient_checkpointing: false
  
# Checkpointing
checkpoint:
  save_best: false
  save_last: true
  resume: null
  keep_last_n: 1
