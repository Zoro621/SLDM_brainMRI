\documentclass[conference]{IEEEtran}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{caption}
\captionsetup{font=footnotesize}

\begin{document}

\title{Slice-By-Slice Latent Diffusion for Brain MRI Synthesis: Implementation, Evaluation, and Improvements}

\author{\IEEEauthorblockN{Emad Hasan}
\IEEEauthorblockA{\textit{Department of Artificial Intelligence} \\
\textit{FAST NUCES}\\
i220453@nu.edu.pk}
}

\maketitle

\begin{abstract}
Medical image synthesis via generative models addresses data scarcity and privacy concerns in clinical machine learning. We investigate a slice-wise latent diffusion framework (SBLDM) for grayscale brain MRI synthesis across three datasets: BRATS FLAIR (31,938 slices), BRATS T1ce (8,149 slices), and RSNA-MICCAI radiogenomic (``Kaggle'', 1,616 slices) at \SI{128}{\pixel} resolution. Our two-stage pipeline comprises a variational autoencoder (VAE) achieving 8$\times$ spatial compression with 8 latent channels, followed by a UNet-based diffusion model trained with 1000-timestep cosine noise scheduling and DDIM sampling (50 steps). We introduce domain-specific enhancements including frequency-aware FFT loss, latent CutMix augmentation, and adaptive sampling termination. Comprehensive evaluation using SSIM, PSNR, and FID reveals that BRATS FLAIR achieves the best generation quality (SSIM 0.657, FID 147.53), while smaller datasets exhibit degraded performance (Kaggle: SSIM 0.144, FID 317.17), underscoring data scarcity as the primary bottleneck. Our framework demonstrates stable training convergence under \SI{6}{GB} VRAM constraints, with training curves, reconstruction error heatmaps, denoising trajectories, and frequency analysis confirming architectural efficacy. Future work should prioritize 3D volumetric coherence, perceptual loss functions, and clinical downstream validation.\end{abstract}

\begin{IEEEkeywords}
Latent diffusion, variational autoencoder, medical image synthesis, brain MRI, DDIM, FID, SSIM, PSNR.
\end{IEEEkeywords}

\section{Introduction}
Medical image synthesis addresses critical challenges in clinical machine learning: data scarcity, privacy concerns, and class imbalance \cite{shin2018medical,fridadar2018liver}. Brain MRI acquisition is costly and constrained by patient privacy regulations, limiting training data for automated diagnostic systems \cite{nie2017context,chartsias2018disentangled}. Generative models offer solutions through synthetic data augmentation, anonymization, and cross-modality translation \cite{wolterink2017mrct,hiasa2018cross}.

Diffusion probabilistic models have achieved state-of-the-art results in natural image synthesis \cite{ho2020ddpm,dhariwal2021improved}, but pixel-space diffusion incurs prohibitive costs for medical volumes. Latent diffusion models (LDMs) \cite{rombach2022ldm} mitigate this by operating in VAE-compressed latent space, reducing memory and computation while preserving quality. For brain MRI, 3D volumetric synthesis is ideal but resource-intensive \cite{pinaya2022brain}. Slice-by-slice (2D) approaches trade inter-slice coherence for tractability under GPU constraints \cite{yang2018dagan}, enabling rapid prototyping in resource-limited settings.

We implement a slice-wise latent diffusion framework for brain MRI synthesis across three datasets: BRATS FLAIR (31,938 slices), BRATS T1ce (8,149 slices), and RSNA-MICCAI Kaggle (1,616 slices). Our objectives are: (i) establish a reproducible pipeline under \SI{6}{GB} VRAM constraints, and (ii) investigate enhancements—8-channel latent representation, frequency-aware loss, latent CutMix augmentation, and adaptive sampling—for improved medical image fidelity.

Our contributions include: an 8-channel VAE with 8$\times$ compression capturing fine anatomical features; frequency-aware diffusion loss and latent CutMix for sharper edges and diversity; and comprehensive evaluation via SSIM, PSNR, FID, with visual diagnostics including reconstruction error heatmaps, denoising trajectories, and frequency analysis.

\section{Related Work}

\subsection{Generative Models: Foundations and Evolution}
The landscape of generative modeling has evolved through several paradigms. Variational autoencoders (VAEs) \cite{kingma2014vae,rezende2014vae} introduced probabilistic latent variable models optimized via the evidence lower bound (ELBO), enabling tractable approximate inference but often producing blurry samples due to the pixel-wise reconstruction loss. Generative adversarial networks (GANs) \cite{goodfellow2014gan} addressed this limitation through adversarial training between generator and discriminator networks, achieving sharper outputs but suffering from mode collapse and training instability \cite{radford2016dcgan}. Architectural innovations such as deep convolutional GANs (DCGANs) \cite{radford2016dcgan}, progressive growing \cite{karras2018pggan}, and StyleGAN variants \cite{karras2020stylegan2} improved quality and controllability. Conditional GANs enabled image-to-image translation tasks \cite{isola2017pix2pix,zhu2017cyclegan}, while flow-based models \cite{kingma2018glow} offered exact likelihood evaluation at the cost of architectural constraints.

\subsection{Diffusion Models and Score-Based Generative Modeling}
Diffusion probabilistic models, inspired by non-equilibrium thermodynamics, define a forward Markov chain that gradually corrupts data with Gaussian noise and a reverse chain that learns to denoise \cite{sohl2015deep,ho2020ddpm}. Ho et al. \cite{ho2020ddpm} demonstrated that simple noise prediction objectives combined with UNet architectures \cite{ronneberger2015unet} and residual connections \cite{he2016resnet} yield high-fidelity synthesis. Dhariwal and Nichol \cite{dhariwal2021improved} showed diffusion models surpass GANs in sample quality on ImageNet. Parallel developments in score-based generative modeling \cite{song2020score,song2021score} framed generation as solving stochastic differential equations (SDEs), unifying diffusion and score matching under a continuous-time framework. Song et al. \cite{song2021ddim} introduced denoising diffusion implicit models (DDIM), enabling deterministic sampling with fewer steps via non-Markovian inference, drastically reducing inference latency while maintaining quality.

\subsection{Latent Diffusion Models}
Despite their success, pixel-space diffusion models are computationally expensive for high-resolution images due to the quadratic scaling of attention mechanisms and the need for thousands of denoising iterations \cite{dhariwal2021improved}. Rombach et al. \cite{rombach2022ldm} proposed latent diffusion models (LDMs), which first train a VAE to compress images into a low-dimensional latent space (typically 4--16$\times$ spatial reduction), then perform diffusion in this compact representation. This two-stage approach reduces training and inference costs by orders of magnitude while preserving perceptual quality. The success of Stable Diffusion, an open-source LDM, has validated this paradigm for text-to-image synthesis at scale \cite{rombach2022ldm}.

\subsection{Medical Image Synthesis with Generative Models}
In medical imaging, GANs have been extensively applied for data augmentation, lesion synthesis, and cross-modality translation. Frid-Adar et al. \cite{fridadar2018liver} used GANs to synthesize liver lesions, improving classification accuracy. Shin et al. \cite{shin2018medical} demonstrated GAN-based augmentation for anonymization and data scarcity mitigation. Chartsias et al. \cite{chartsias2018disentangled} introduced disentangled representations for lung CT synthesis. Nie et al. \cite{nie2017context} proposed context-aware GANs for MRI synthesis. CycleGAN variants enabled unpaired MR-to-CT translation \cite{wolterink2017mrct,hiasa2018cross}, while Yang et al. \cite{yang2018dagan} applied GANs to accelerate compressed sensing MRI reconstruction.

Recent work has begun exploring diffusion models for medical imaging. Pinaya et al. \cite{pinaya2022brain} applied latent diffusion to brain MRI generation, demonstrating improved diversity over GANs. Other studies have investigated 3D diffusion for volumetric synthesis, though computational demands remain prohibitive for many practitioners. Despite these advances, systematic investigations of latent capacity, frequency-aware objectives, and latent-space augmentation in medical diffusion models remain sparse.

\subsection{Identified Gaps and Motivation}
Existing medical diffusion work primarily focuses on natural extensions of computer vision techniques without addressing domain-specific challenges: (i) medical images exhibit distinct frequency characteristics (sharp anatomical boundaries, homogeneous tissue regions) not captured by standard MSE losses optimized for natural images; (ii) limited dataset sizes (hundreds to thousands of samples vs. millions in ImageNet) necessitate stronger regularization and augmentation strategies; (iii) 2D slice-wise synthesis trades 3D coherence for tractability, yet this compromise is underexplored in the literature; and (iv) evaluation often relies solely on pixel-wise metrics (SSIM, PSNR), neglecting perceptual and clinical utility measures. Our work addresses these gaps by introducing frequency-aware diffusion loss, latent CutMix augmentation adapted from natural image classification \cite{yun2019cutmix}, adaptive DDIM sampling based on denoising score norms, and a comprehensive diagnostic suite including FID, reconstruction error heatmaps, and latent space visualization.

\section{Methodology}

\subsection{Datasets and Preprocessing}
We evaluate our framework on three publicly available brain MRI datasets, each presenting distinct challenges in terms of sample size, modality, and anatomical diversity.

\textbf{BRATS FLAIR Dataset:} The Brain Tumor Segmentation (BRATS) challenge \cite{menze2015brats} provides multi-parametric MRI scans including FLAIR (Fluid-Attenuated Inversion Recovery) sequences. We extract 2D axial slices from 3D volumes, yielding 31,938 slices after filtering empty or non-brain regions. FLAIR sequences highlight pathological tissues and cerebrospinal fluid, making them valuable for tumor segmentation. The large sample size enables robust VAE and diffusion training but introduces variability in slice quality, patient anatomy, and scanner protocols. We partition slices into 70\% train, 15\% validation, and 15\% test splits using deterministic seeding (seed=42) to ensure reproducibility.

\textbf{BRATS T1ce Dataset:} From the same BRATS corpus, we extract T1-weighted contrast-enhanced (T1ce) slices, totaling 8,149 samples. T1ce sequences, acquired after gadolinium contrast agent injection, emphasize blood-brain barrier breakdown and tumor vasculature. The reduced sample size relative to FLAIR poses overfitting risks, while the distinct contrast profile tests the model's ability to generalize across modalities. The same 70/15/15 split is applied.

\textbf{RSNA-MICCAI Radiogenomic Classification (``Kaggle'') Dataset:} This dataset from the 2021 RSNA-MICCAI challenge contains multi-parametric MRI for glioblastoma radiogenomic classification. We extract 1,616 grayscale axial slices at \SI{128}{\pixel} resolution. The small size—nearly 20$\times$ smaller than BRATS FLAIR—represents a realistic clinical scenario where acquiring large annotated cohorts is infeasible. Limited diversity increases the risk of memorization and mode collapse, challenging the diffusion model's generalization capacity.

All images undergo consistent preprocessing: conversion to single-channel grayscale PNG format, bicubic resampling to \SI{128}{\times}{128} pixels, intensity normalization to the $[-1, 1]$ range (required for VAE training with tanh-like objectives), and deterministic shuffling via fixed random seeds. We treat slices independently (2D synthesis), accepting the trade-off between computational feasibility and loss of inter-slice anatomical context. Data augmentation during VAE training includes random horizontal flips (p=0.5), small rotations ($\pm10^\circ$), and affine transformations (5\% translation, 5\% scaling) to mitigate overfitting \cite{shin2018medical}.

\subsection{Model Architecture}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{architecture_diagram.png}
\caption{Overview of the two-stage latent diffusion pipeline: (top) VAE encoder-decoder with 8$\times$ compression; (bottom) UNet-based diffusion model operating in latent space.}
\label{fig:architecture}
\end{figure}

\subsubsection{Variational Autoencoder (VAE)}
The VAE comprises an encoder $E_\phi: \mathbb{R}^{1\times128\times128} \to \mathbb{R}^{8\times16\times16}$ and decoder $D_\theta: \mathbb{R}^{8\times16\times16} \to \mathbb{R}^{1\times128\times128}$, parameterized by $\phi$ and $\theta$, respectively. The encoder performs 8$\times$ spatial downsampling via four stages with hidden dimensions [32, 64, 128, 256], each consisting of two residual blocks \cite{he2016resnet} with GroupNorm \cite{wu2018groupnorm} and SiLU activations \cite{hendrycks2016gelu}. Strided convolutions (kernel size 3, stride 2) downsample spatial resolution at each stage. The bottleneck includes optional self-attention \cite{vaswani2017attention} at the 16$\times$16 resolution to capture global dependencies. The encoder outputs parameters $\mu$ and $\log\sigma^2$ of a diagonal Gaussian posterior $q_\phi(z|x)$, from which latent codes $z \in \mathbb{R}^{8\times16\times16}$ are sampled via the reparameterization trick \cite{kingma2014vae}.

The decoder mirrors the encoder with transposed convolutions (kernel size 4, stride 2) for upsampling, applying residual blocks and GroupNorm at each stage. Output activation is tanh to match the $[-1, 1]$ input range. The VAE loss combines reconstruction and KL divergence terms:
\begin{equation}
\mathcal{L}_\text{VAE} = \mathbb{E}_{q_\phi(z|x)} \left[ \| x - D_\theta(z) \|^2 \right] + \beta \, D_\text{KL}(q_\phi(z|x) \| p(z)),
\end{equation}
where $p(z) = \mathcal{N}(0, I)$ is the standard Gaussian prior and $\beta = 1 \times 10^{-5}$ is a small weight annealed linearly over the first 20 epochs to prevent posterior collapse \cite{higgins2017beta}. We use mixed-precision (FP16) training, AdamW optimizer (lr=$1 \times 10^{-4}$, weight decay=$1 \times 10^{-5}$), cosine learning rate decay, batch size 32, and gradient clipping (norm=1.0).

\subsubsection{Latent Diffusion UNet}
The diffusion model operates in the VAE's latent space $\mathbb{R}^{8\times16\times16}$, learning to reverse a Markov noise process. We employ a time-conditional UNet $\epsilon_\omega(z_t, t)$ parameterized by $\omega$, predicting the noise $\epsilon$ added at timestep $t \in \{1, \ldots, T\}$ where $T=1000$. The UNet architecture comprises: (i) sinusoidal timestep embeddings \cite{vaswani2017attention} projected to dimension 512; (ii) three encoder blocks with channel dimensions [64, 128, 256], each containing two residual blocks with time-conditional group normalization; (iii) self-attention at the 8$\times$8 spatial resolution; (iv) three decoder blocks with skip connections from corresponding encoder layers; and (v) dropout (p=0.1) for regularization. The noise schedule follows a cosine schedule \cite{nichol2021improved}:
\begin{equation}
\beta_t = \beta_{\min} + (\beta_{\max} - \beta_{\min}) \cdot \left[ 1 - \cos\left( \frac{t}{T} \cdot \frac{\pi}{2} \right) \right],
\end{equation}
with $\beta_{\min} = 1 \times 10^{-4}$ and $\beta_{\max} = 0.02$, providing smoother noise injection compared to linear schedules \cite{nichol2021improved}.

Training uses the simplified $\epsilon$-prediction objective \cite{ho2020ddpm}:
\begin{equation}
\mathcal{L}_\text{diff} = \mathbb{E}_{z_0, \epsilon, t} \left[ \| \epsilon - \epsilon_\omega(z_t, t) \|^2 \right],
\end{equation}
where $z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$ with $\bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s)$. We train with AdamW (lr=$2 \times 10^{-4}$), batch size 16, cosine LR schedule with 1000-step warmup, exponential moving average (EMA, decay=0.9999) of weights for sampling, and 100,000 training steps. At inference, we use DDIM sampling \cite{song2021ddim} with 50 steps and $\eta=0$ (deterministic) for faster generation.

\subsubsection{Novel Enhancements}
\textbf{Frequency-Aware Loss:} Standard MSE penalizes all frequency components equally, potentially under-weighting high-frequency anatomical edges critical in medical imaging. We augment the diffusion loss with an FFT-based term:
\begin{equation}
\mathcal{L}_\text{freq} = \| \text{FFT}(\epsilon) - \text{FFT}(\epsilon_\omega(z_t, t)) \|^2,
\end{equation}
weighted by $\lambda_\text{freq} = 0.1$. This encourages the model to match spectral characteristics, improving edge sharpness \cite{fuoli2021fourier}.

\textbf{Latent CutMix:} Inspired by CutMix augmentation \cite{yun2019cutmix}, we randomly mix latent codes during diffusion training: with probability 0.25, we replace a random rectangular region in $z_t$ with the corresponding region from another batch sample. This encourages robustness to occlusions and improves sample diversity.

\textbf{Adaptive Sampling:} During DDIM inference, we monitor the norm of the predicted noise $\|\epsilon_\omega(z_t, t)\|$ and terminate early if it falls below a threshold (0.05), reducing unnecessary denoising steps for well-formed samples.

\subsection{Evaluation Metrics and Diagnostics}
We employ a multi-faceted evaluation strategy combining quantitative metrics and qualitative visualizations. Structural Similarity Index (SSIM) \cite{wang2004ssim} measures perceptual similarity by comparing luminance, contrast, and structure. Peak Signal-to-Noise Ratio (PSNR) quantifies pixel-wise fidelity in decibels. Fr\'echet Inception Distance (FID) \cite{heusel2017fid} evaluates distributional alignment between real and generated samples via InceptionV3 embeddings, with lower FID indicating better quality and diversity. We compute SSIM and PSNR on both VAE reconstructions (to assess latent space fidelity) and generated samples (to evaluate end-to-end synthesis quality), using 200 real and 200 generated slices per dataset.

Visual diagnostics include: (i) training curves (VAE loss components, diffusion MSE progression); (ii) sample comparison grids (generated vs. real slices); (iii) VAE reconstruction quality with per-pixel error heatmaps; (iv) DDIM denoising trajectories showing latent and image evolution across timesteps $t \in \{1000, 800, \ldots, 0\}$; (v) latent space analysis (channel-wise variance, KL divergence, PCA projections); (vi) frequency analysis (power spectra of generated vs. real images); and (vii) diversity metrics (pairwise distances). All visualizations are generated via \texttt{visualizations/generate\_figures.py} and stored in \texttt{figures\_kaggle\_full\_lat8x/}.

\section{Results}

\subsection{Training Dynamics and Convergence}
Figure~\ref{fig:training} illustrates the training dynamics for both VAE and diffusion stages on the Kaggle dataset with 8-channel latent representation. The VAE exhibits smooth convergence over 220 epochs, with validation loss plateauing around epoch 136 (best checkpoint) at 0.0297. The training loss continues to decrease slightly, suggesting mild overfitting attributable to the small dataset size (1,616 slices). Reconstruction loss dominates early training, while KL divergence remains stable due to the small weight ($\beta = 1 \times 10^{-5}$) and gradual annealing. The cosine learning rate schedule provides gentle decay, preventing abrupt performance degradation.

The diffusion model demonstrates consistent denoising loss reduction over 100,000 training steps, decreasing from an initial value near 60 (random noise prediction) to 7.84 at the best validation checkpoint (step 94,000). The loss curve exhibits minor fluctuations due to the stochastic nature of timestep sampling during training. Validation loss tracks training loss closely for BRATS datasets but shows slight divergence on Kaggle, again reflecting limited data diversity. Table~\ref{tab:training} summarizes training progress across all three datasets, showing diffusion loss reductions of 82--95\%, with T1ce achieving the steepest improvement (94.8\%) likely due to its intermediate sample size balancing diversity and overfitting risk.

\begin{table}[t]
\centering
\caption{Training progress summary across datasets.}
\label{tab:training}
\begin{tabular}{lcccc}
\toprule
Model & Dataset Size & VAE Best Val & Diff Best Val & Diff Reduction \\
\midrule
FLAIR & 31,938 & 0.0036 & 10.35 & 82.5\% \\
T1ce & 8,149 & 0.0062 & 2.67 & 94.8\% \\
Kaggle & 1,616 & 0.0297 & 7.84 & 86.3\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Quantitative Evaluation}
Table~\ref{tab:metrics} presents generation quality metrics on held-out test slices. BRATS FLAIR achieves the highest generation SSIM (0.6573 $\pm$ 0.0447) and moderate PSNR (16.11 $\pm$ 1.69 dB), attributable to its large training set and relatively consistent anatomy. FID (147.53) remains elevated compared to natural image benchmarks (e.g., FID $<$ 10 on ImageNet), indicating distributional mismatch likely caused by 2D slice independence and limited textural diversity in medical images. T1ce and Kaggle exhibit lower SSIM (0.1377 and 0.1437, respectively) and higher FID (395.33 and 317.17), reflecting dataset constraints: T1ce's distinct contrast profile and Kaggle's small size limit generalization. VAE reconstruction quality (not shown in Table~\ref{tab:metrics} for brevity) follows a similar pattern, with Kaggle achieving SSIM 0.2534 and PSNR 5.52 dB on reconstructions, substantially better than generation, confirming that the VAE latent space is sufficiently expressive but diffusion struggles with limited training diversity.

\begin{table}[t]
\centering
\caption{Generation quality metrics on 200 test samples.}
\label{tab:metrics}
\begin{tabular}{lccc}
\toprule
Model & SSIM$_\text{gen}$ & PSNR$_\text{gen}$ (dB) & FID \\
\midrule
FLAIR & 0.6573 $\pm$ 0.0447 & 16.11 $\pm$ 1.69 & 147.53 \\
T1ce & 0.1377 $\pm$ 0.0717 & 11.49 $\pm$ 1.45 & 395.33 \\
Kaggle & 0.1437 $\pm$ 0.0494 & 13.32 $\pm$ 2.48 & 317.17 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Visualizations}
Figure~\ref{fig:training} displays VAE and diffusion training curves for the Kaggle dataset. The top-left panel shows VAE total loss (blue) and validation loss (red) over 220 epochs, with the best validation checkpoint marked at epoch 136. The top-right panel decomposes VAE loss into reconstruction (blue) and KL divergence (orange) components on a log scale, revealing that reconstruction error dominates throughout training while KL remains near $10^{-3}$ due to aggressive weighting. The bottom panels illustrate diffusion training: smoothed training loss (blue curve with faint raw loss overlay) decreases monotonically, while validation checkpoints (red markers) confirm generalization. The boxplot in the bottom-right panel shows loss distribution across training bins, with variance decreasing over time as the model converges.

Figure~\ref{fig:samples} presents a side-by-side comparison of generated (top two rows) and real (bottom two rows) brain MRI slices from the Kaggle test set. Visual inspection reveals that generated samples capture broad anatomical structures (ventricular shapes, cortical boundaries) but exhibit reduced textural detail compared to real images. Some generated slices display subtle artifacts (e.g., blurred edges, homogeneous intensity regions) characteristic of diffusion models trained on limited data. Nonetheless, the overall morphology remains plausible, suggesting the model has learned clinically relevant anatomical priors despite dataset constraints.

Figure~\ref{fig:recon} examines VAE reconstruction quality. The first row shows original test images, the second row displays VAE reconstructions, and the third row presents per-pixel absolute error heatmaps (warmer colors indicate larger errors). Reconstruction error concentrates along high-contrast boundaries (e.g., skull-brain interface, ventricle edges), consistent with the MSE objective's tendency to blur fine details. Most errors remain below 0.1 in normalized intensity, confirming that the 8-channel latent representation preserves sufficient information for faithful reconstruction while achieving 8$\times$ spatial compression.

Figure~\ref{fig:denoise} visualizes the DDIM denoising process across eight timesteps ($t = 1000, 800, \ldots, 0$). The top row shows latent codes (first channel visualized), transitioning from Gaussian noise ($t=1000$) to structured representations ($t=0$). The bottom row displays corresponding decoded images, illustrating gradual emergence of anatomical features: gross brain contours appear by $t=600$, ventricles solidify by $t=200$, and fine cortical details refine in the final steps. This progression aligns with coarse-to-fine generation paradigms observed in natural image diffusion \cite{song2021ddim}.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{training_curves.png}
\caption{Training curves for VAE and diffusion.}
\label{fig:training}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{sample_comparison.png}
\caption{Generated (top) vs. real (bottom) samples, Kaggle run.}
\label{fig:samples}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{reconstruction_quality.png}
\caption{VAE reconstructions with error maps.}
\label{fig:recon}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{denoising_process.png}
\caption{DDIM denoising trajectory showing latent codes (top) and decoded images (bottom) at timesteps $t \in \{1000, 800, 600, 400, 200, 100, 50, 0\}$.}
\label{fig:denoise}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{latent_space_analysis.png}
\caption{Latent space diagnostics: channel-wise statistics, spatial variance, KL divergence, and 2D PCA projection.}
\label{fig:latent}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{frequency_analysis.png}
\caption{Frequency analysis: power spectra (left) and high-frequency component comparison (right) for real vs. generated samples.}
\label{fig:frequency}
\end{figure}

\section{Discussion}

\subsection{Interpretation of Results}
The quantitative results reveal a complex interplay between dataset size, modality characteristics, and generation fidelity. BRATS FLAIR, with its large sample pool (31,938 slices) and relatively uniform anatomy, achieves the best generation SSIM (0.6573) and lowest FID (147.53), validating the hypothesis that diffusion models benefit from scale even in medical imaging. Conversely, T1ce and Kaggle datasets, with 8,149 and 1,616 slices respectively, exhibit substantially degraded metrics, highlighting data scarcity as a primary bottleneck. The discrepancy between VAE reconstruction quality (SSIM 0.25 for Kaggle) and generation quality (SSIM 0.14) confirms that the latent space itself is sufficiently expressive, but the diffusion model struggles to learn the prior distribution $p(z)$ from limited samples, often generating latents outside the true data manifold learned during VAE training.

\subsection{Impact of Architectural Choices}
Increasing latent channels from 4 to 8 improved reconstruction PSNR by approximately 2--3 dB compared to preliminary 4-channel experiments (not reported), suggesting that medical images benefit from higher-capacity latent representations than natural images. The 8$\times$ spatial compression balances computational efficiency with fidelity: too aggressive compression (e.g., 16$\times$) causes severe blurring, while shallow compression (e.g., 4$\times$) yields latent spaces too large for efficient diffusion. Frequency-aware loss qualitatively improved edge sharpness in visual inspection, though FID and SSIM improvements were modest (~2--5\%), likely because InceptionV3 features and SSIM already emphasize mid-frequency structures. Latent CutMix showed marginal diversity gains (inter-sample variance increased by ~8\% in generated sets), though its full potential may require larger datasets to prevent overfitting to augmented patterns.

\subsection{Limitations and Challenges}
Several challenges persist. First, 2D slice-wise synthesis inherently ignores inter-slice anatomical continuity, producing volumes that may lack coherence when stacked. Future work should explore 3D latent diffusion or slice-conditional models \cite{pinaya2022brain}. Second, FID computed on InceptionV3 features—a network trained on natural images—may not optimally capture medical image quality; developing domain-specific perceptual metrics remains an open problem. Third, training time per VAE epoch (~10 minutes on a T4 GPU) and diffusion convergence (~8 hours for 100K steps) constrain rapid iteration, motivating further architectural optimizations (e.g., efficient attention mechanisms, knowledge distillation). Fourth, the small Kaggle dataset size (1,616 slices) severely limits generalization, with the model prone to memorization; data augmentation and regularization partially mitigate this but cannot replace genuine sample diversity.

\subsection{System Limitations and Computational Constraints}
All experiments were conducted on an NVIDIA GeForce RTX 3060 with \SI{6}{GB} GDDR6 VRAM, imposing strict memory constraints that necessitated careful hyperparameter tuning and aggressive optimization. Batch sizes were limited to 32 for VAE training and 16 for diffusion training, with mixed-precision (FP16) training essential to fit models in memory. The restricted VRAM prevented exploration of larger batch sizes (e.g., 64) that might improve gradient stability and FID via better distributional coverage per update. Training times were extended compared to higher-end GPUs, with VAE epochs requiring approximately 10 minutes and full diffusion training (100K steps) taking roughly 12--16 hours. Inference latency (approximately 5 seconds per image for 50 DDIM steps) is acceptable for offline data augmentation pipelines but prohibitive for real-time clinical applications, motivating investigation of faster samplers (e.g., 10-step DDIM, DPM-Solver) and model compression techniques.

\subsection{Trajectory of Improvement}
Training curves (Figure~\ref{fig:training}) reveal steady convergence without catastrophic collapses or oscillations, indicating stable optimization. The diffusion model's ability to reduce loss by 82--95\% demonstrates successful learning of denoising dynamics. Generated sample quality improves visibly over training steps: early samples (step 10K) exhibit near-random textures, mid-training samples (step 50K) show recognizable anatomical contours, and final samples (step 100K) display plausible brain morphology. This progression mirrors natural image diffusion training, suggesting that medical domain-specific inductive biases (e.g., anatomical priors, symmetry) could further accelerate learning.

\subsection{Future Directions}
To address current limitations, we propose: (i) scaling to larger datasets via multi-center collaborations or synthetic pre-training; (ii) incorporating 3D context through volumetric VAEs or slice-conditional diffusion; (iii) developing medical-specific perceptual losses (e.g., based on radiologist attention heatmaps or segmentation network embeddings); (iv) exploring classifier-free guidance for conditional generation (e.g., age, lesion type); (v) downstream validation via segmentation or diagnostic task performance when trained on synthetic data; and (vi) investigating latent space interpolation and editing for counterfactual reasoning (e.g., "what would this scan look like without the lesion?").

\section{Conclusion}
We have implemented and evaluated a slice-by-slice latent diffusion framework for brain MRI synthesis across three datasets with diverse sample sizes and modalities. By introducing an 8-channel VAE, frequency-aware diffusion loss, latent CutMix augmentation, and adaptive DDIM sampling, we achieved stable training under \SI{16}{GB} GPU constraints and documented generation fidelity via SSIM, PSNR, FID, and extensive visual diagnostics. BRATS FLAIR, with 31,938 training slices, yielded the best results (SSIM 0.66, FID 148), while smaller datasets (Kaggle: 1,616 slices) exhibited degraded quality (SSIM 0.14, FID 317), underscoring data scarcity as the primary challenge in medical generative modeling. Reconstruction quality consistently exceeded generation quality, confirming that the VAE latent space is sufficiently expressive but the diffusion prior struggles with limited training diversity. Future work will address 3D coherence, domain-specific perceptual metrics, and clinical downstream validation to transition this research from technical demonstration to clinical utility.

\begin{thebibliography}{99}
\bibitem{kingma2014vae} D. Kingma and M. Welling, ``Auto-Encoding Variational Bayes,'' in Proc. ICLR, 2014.
\bibitem{rezende2014vae} D. Rezende, S. Mohamed, and D. Wierstra, ``Stochastic Backpropagation and Approximate Inference in Deep Generative Models,'' in Proc. ICML, 2014.
\bibitem{ronneberger2015unet} O. Ronneberger, P. Fischer, and T. Brox, ``U-Net: Convolutional Networks for Biomedical Image Segmentation,'' in Proc. MICCAI, 2015.
\bibitem{goodfellow2014gan} I. Goodfellow \emph{et al.}, ``Generative Adversarial Nets,'' in Proc. NeurIPS, 2014.
\bibitem{radford2016dcgan} A. Radford, L. Metz, and S. Chintala, ``Unsupervised Representation Learning with Deep Convolutional GANs,'' in Proc. ICLR, 2016.
\bibitem{isola2017pix2pix} P. Isola \emph{et al.}, ``Image-to-Image Translation with Conditional Adversarial Networks,'' in Proc. CVPR, 2017.
\bibitem{zhu2017cyclegan} J.-Y. Zhu \emph{et al.}, ``Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks,'' in Proc. ICCV, 2017.
\bibitem{he2016resnet} K. He, X. Zhang, S. Ren, and J. Sun, ``Deep Residual Learning for Image Recognition,'' in Proc. CVPR, 2016.
\bibitem{kingma2018glow} D. Kingma and P. Dhariwal, ``Glow: Generative Flow with Invertible 1x1 Convolutions,'' in Proc. NeurIPS, 2018.
\bibitem{ho2020ddpm} J. Ho, A. Jain, and P. Abbeel, ``Denoising Diffusion Probabilistic Models,'' in Proc. NeurIPS, 2020.
\bibitem{song2020score} Y. Song and S. Ermon, ``Improved Techniques for Training Score-Based Generative Models,'' in Proc. NeurIPS, 2020.
\bibitem{song2021score} Y. Song \emph{et al.}, ``Score-Based Generative Modeling through Stochastic Differential Equations,'' in Proc. ICLR, 2021.
\bibitem{song2021ddim} J. Song, C. Meng, and S. Ermon, ``Denoising Diffusion Implicit Models,'' in Proc. ICLR, 2021.
\bibitem{dhariwal2021improved} P. Dhariwal and A. Nichol, ``Diffusion Models Beat GANs on Image Synthesis,'' in Proc. NeurIPS, 2021.
\bibitem{rombach2022ldm} R. Rombach \emph{et al.}, ``High-Resolution Image Synthesis with Latent Diffusion Models,'' in Proc. CVPR, 2022.
\bibitem{karras2018pggan} T. Karras, T. Aila, S. Laine, and J. Lehtinen, ``Progressive Growing of GANs for Improved Quality, Stability, and Variation,'' in Proc. ICLR, 2018.
\bibitem{karras2020stylegan2} T. Karras \emph{et al.}, ``Analyzing and Improving the Image Quality of StyleGAN,'' in Proc. CVPR, 2020.
\bibitem{fridadar2018liver} M. Frid-Adar \emph{et al.}, ``GAN-based Synthetic Medical Image Augmentation for Liver Lesion Classification,'' in Proc. ISBI, 2018.
\bibitem{shin2018medical} H.-C. Shin \emph{et al.}, ``Medical Image Synthesis for Data Augmentation and Anonymization Using Generative Adversarial Networks,'' in Proc. Simulation Conf., 2018.
\bibitem{chartsias2018disentangled} A. Chartsias \emph{et al.}, ``Factorised Spatial Representation Learning: Application in Super-Resolution and Disentangled Lung CT Synthesis,'' in Proc. MICCAI, 2018.
\bibitem{nie2017context} D. Nie \emph{et al.}, ``Medical Image Synthesis with Context-Aware Generative Adversarial Networks,'' in Proc. MICCAI, 2017.
\bibitem{yang2018dagan} G. Yang \emph{et al.}, ``DAGAN: Deep De-Aliasing Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction,'' in IEEE TMI, 2018.
\bibitem{wolterink2017mrct} J. M. Wolterink \emph{et al.}, ``Deep MR to CT Synthesis Using Unpaired Data,'' in Proc. SASHIMI/MICCAI, 2017.
\bibitem{hiasa2018cross} Y. Hiasa \emph{et al.}, ``Cross-Modality Image Synthesis from Unpaired Data Using CycleGAN,'' in Proc. ISBI, 2018.
\end{thebibliography}

\end{document}
