{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da51fea",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e595107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if needed)\n",
    "!pip install -q einops pytorch-fid\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from PIL import Image\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4505d",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de6ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick debug config - use for testing\n",
    "CONFIG = {\n",
    "    'experiment': {\n",
    "        'name': 'sbldm_kaggle',\n",
    "        'seed': 42,\n",
    "    },\n",
    "    'data': {\n",
    "        'resolution': 128,\n",
    "        'channels': 1,\n",
    "    },\n",
    "    'vae': {\n",
    "        'in_channels': 1,\n",
    "        'latent_channels': 4,\n",
    "        'hidden_dims': [32, 64, 128, 256],\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 1e-4,\n",
    "        'epochs': 20,  # Quick run\n",
    "        'kl_weight': 1e-5,\n",
    "    },\n",
    "    'unet': {\n",
    "        'in_channels': 4,\n",
    "        'out_channels': 4,\n",
    "        'model_channels': 64,\n",
    "        'channel_mult': [1, 2, 4],\n",
    "        'num_res_blocks': 2,\n",
    "        'attention_resolutions': [8],\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    'diffusion': {\n",
    "        'timesteps': 1000,\n",
    "        'beta_schedule': 'cosine',\n",
    "        'beta_start': 1e-4,\n",
    "        'beta_end': 0.02,\n",
    "        'gamma': 1.0,  # For noise rebalancing\n",
    "        'use_freq_loss': True,\n",
    "        'freq_loss_weight': 0.1,\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 2e-4,\n",
    "        'training_steps': 5000,  # Quick run\n",
    "        'ema_decay': 0.9999,\n",
    "    },\n",
    "    'sampling': {\n",
    "        'ddim_steps': 50,\n",
    "        'eta': 0.0,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dad786",
   "metadata": {},
   "source": [
    "## 3. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2602fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VAE Components\n",
    "# ============================================================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, groups=8):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(min(groups, in_channels), in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(min(groups, out_channels), out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.skip = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.norm1(x))\n",
    "        h = self.conv1(h)\n",
    "        h = self.act(self.norm2(h))\n",
    "        h = self.conv2(h)\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(8, channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(channels, channels, 1)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.norm(x)\n",
    "        qkv = self.qkv(h).view(B, 3, self.num_heads, self.head_dim, H * W)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
    "        attn = torch.einsum('bhdn,bhdm->bhnm', q, k) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.einsum('bhnm,bhdm->bhdn', attn, v).view(B, C, H, W)\n",
    "        return x + self.proj(out)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels=1, latent_channels=4, hidden_dims=[32, 64, 128, 256]):\n",
    "        super().__init__()\n",
    "        self.latent_channels = latent_channels\n",
    "        self.downsample_factor = 2 ** (len(hidden_dims) - 1)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = [nn.Conv2d(in_channels, hidden_dims[0], 3, padding=1)]\n",
    "        in_dim = hidden_dims[0]\n",
    "        for i, out_dim in enumerate(hidden_dims):\n",
    "            encoder_layers.extend([\n",
    "                ResidualBlock(in_dim, out_dim),\n",
    "                ResidualBlock(out_dim, out_dim),\n",
    "            ])\n",
    "            if i < len(hidden_dims) - 1:\n",
    "                encoder_layers.append(nn.Conv2d(out_dim, out_dim, 3, stride=2, padding=1))\n",
    "            in_dim = out_dim\n",
    "        \n",
    "        encoder_layers.extend([\n",
    "            ResidualBlock(hidden_dims[-1], hidden_dims[-1]),\n",
    "            AttentionBlock(hidden_dims[-1]),\n",
    "            ResidualBlock(hidden_dims[-1], hidden_dims[-1]),\n",
    "            nn.GroupNorm(8, hidden_dims[-1]),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(hidden_dims[-1], latent_channels * 2, 3, padding=1)\n",
    "        ])\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_dims = list(reversed(hidden_dims))\n",
    "        decoder_layers = [\n",
    "            nn.Conv2d(latent_channels, decoder_dims[0], 3, padding=1),\n",
    "            ResidualBlock(decoder_dims[0], decoder_dims[0]),\n",
    "            AttentionBlock(decoder_dims[0]),\n",
    "            ResidualBlock(decoder_dims[0], decoder_dims[0]),\n",
    "        ]\n",
    "        \n",
    "        in_dim = decoder_dims[0]\n",
    "        for i, out_dim in enumerate(decoder_dims):\n",
    "            decoder_layers.extend([\n",
    "                ResidualBlock(in_dim, out_dim),\n",
    "                ResidualBlock(out_dim, out_dim),\n",
    "            ])\n",
    "            if i < len(decoder_dims) - 1:\n",
    "                decoder_layers.append(nn.ConvTranspose2d(out_dim, out_dim, 4, stride=2, padding=1))\n",
    "            in_dim = out_dim\n",
    "        \n",
    "        decoder_layers.extend([\n",
    "            nn.GroupNorm(8, decoder_dims[-1]),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(decoder_dims[-1], in_channels, 3, padding=1)\n",
    "        ])\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mean + std * eps\n",
    "        return mean\n",
    "    \n",
    "    def forward(self, x, sample_posterior=True):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar) if sample_posterior else mean\n",
    "        recon = self.decode(z)\n",
    "        return recon, mean, logvar\n",
    "    \n",
    "    def get_latent(self, x, sample=False):\n",
    "        mean, logvar = self.encode(x)\n",
    "        return self.reparameterize(mean, logvar) if sample else mean\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_divergence(mean, logvar):\n",
    "        return -0.5 * torch.mean(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    \n",
    "    def loss_function(self, x, recon, mean, logvar, kl_weight=1e-5):\n",
    "        recon_loss = F.mse_loss(recon, x)\n",
    "        kl_loss = self.kl_divergence(mean, logvar)\n",
    "        loss = recon_loss + kl_weight * kl_loss\n",
    "        return loss, {'loss': loss.item(), 'recon_loss': recon_loss.item(), 'kl_loss': kl_loss.item()}\n",
    "\n",
    "print(\"VAE defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92234201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UNet Score Model\n",
    "# ============================================================================\n",
    "\n",
    "import math\n",
    "\n",
    "def get_timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(max_period) * torch.arange(0, half, device=timesteps.device) / half)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    return torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "\n",
    "class TimeResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(8, in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.time_proj = nn.Sequential(nn.SiLU(), nn.Linear(time_dim, out_ch * 2))\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        h = F.silu(self.norm1(x))\n",
    "        h = self.conv1(h)\n",
    "        t_emb = self.time_proj(t)[:, :, None, None]\n",
    "        scale, shift = t_emb.chunk(2, dim=1)\n",
    "        h = self.norm2(h) * (1 + scale) + shift\n",
    "        h = self.dropout(F.silu(h))\n",
    "        h = self.conv2(h)\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=4, out_ch=4, model_ch=64, ch_mult=[1,2,4], num_res=2, attn_res=[8], dropout=0.0):\n",
    "        super().__init__()\n",
    "        time_dim = model_ch * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(model_ch, time_dim), nn.SiLU(), nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "        self.conv_in = nn.Conv2d(in_ch, model_ch, 3, padding=1)\n",
    "        \n",
    "        # Encoder\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.down_samples = nn.ModuleList()\n",
    "        ch = model_ch\n",
    "        self.skip_channels = [ch]\n",
    "        \n",
    "        for i, mult in enumerate(ch_mult):\n",
    "            out_ch_level = model_ch * mult\n",
    "            for _ in range(num_res):\n",
    "                self.down_blocks.append(TimeResBlock(ch, out_ch_level, time_dim, dropout))\n",
    "                ch = out_ch_level\n",
    "                self.skip_channels.append(ch)\n",
    "            if i < len(ch_mult) - 1:\n",
    "                self.down_samples.append(nn.Conv2d(ch, ch, 3, stride=2, padding=1))\n",
    "                self.skip_channels.append(ch)\n",
    "            else:\n",
    "                self.down_samples.append(None)\n",
    "        \n",
    "        # Middle\n",
    "        self.mid_block1 = TimeResBlock(ch, ch, time_dim, dropout)\n",
    "        self.mid_attn = AttentionBlock(ch)\n",
    "        self.mid_block2 = TimeResBlock(ch, ch, time_dim, dropout)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        self.up_samples = nn.ModuleList()\n",
    "        \n",
    "        for i, mult in enumerate(reversed(ch_mult)):\n",
    "            out_ch_level = model_ch * mult\n",
    "            for j in range(num_res + 1):\n",
    "                skip_ch = self.skip_channels.pop()\n",
    "                self.up_blocks.append(TimeResBlock(ch + skip_ch, out_ch_level, time_dim, dropout))\n",
    "                ch = out_ch_level\n",
    "            if i < len(ch_mult) - 1:\n",
    "                self.up_samples.append(nn.Sequential(\n",
    "                    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                    nn.Conv2d(ch, ch, 3, padding=1)\n",
    "                ))\n",
    "            else:\n",
    "                self.up_samples.append(None)\n",
    "        \n",
    "        self.norm_out = nn.GroupNorm(8, ch)\n",
    "        self.conv_out = nn.Conv2d(ch, out_ch, 3, padding=1)\n",
    "        nn.init.zeros_(self.conv_out.weight)\n",
    "        nn.init.zeros_(self.conv_out.bias)\n",
    "        \n",
    "        self.model_ch = model_ch\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t_emb = get_timestep_embedding(t, self.model_ch)\n",
    "        t_emb = self.time_embed(t_emb)\n",
    "        \n",
    "        h = self.conv_in(x)\n",
    "        skips = [h]\n",
    "        \n",
    "        block_idx = 0\n",
    "        for i, ds in enumerate(self.down_samples):\n",
    "            for _ in range(2):  # num_res\n",
    "                h = self.down_blocks[block_idx](h, t_emb)\n",
    "                skips.append(h)\n",
    "                block_idx += 1\n",
    "            if ds is not None:\n",
    "                h = ds(h)\n",
    "                skips.append(h)\n",
    "        \n",
    "        h = self.mid_block1(h, t_emb)\n",
    "        h = self.mid_attn(h)\n",
    "        h = self.mid_block2(h, t_emb)\n",
    "        \n",
    "        block_idx = 0\n",
    "        for i, us in enumerate(self.up_samples):\n",
    "            for _ in range(3):  # num_res + 1\n",
    "                h = torch.cat([h, skips.pop()], dim=1)\n",
    "                h = self.up_blocks[block_idx](h, t_emb)\n",
    "                block_idx += 1\n",
    "            if us is not None:\n",
    "                h = us(h)\n",
    "        \n",
    "        h = F.silu(self.norm_out(h))\n",
    "        return self.conv_out(h)\n",
    "\n",
    "print(\"UNet defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Diffusion Utilities\n",
    "# ============================================================================\n",
    "\n",
    "class NoiseSchedule:\n",
    "    def __init__(self, schedule_type='cosine', timesteps=1000, beta_start=1e-4, beta_end=0.02, gamma=1.0, device='cuda'):\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        if schedule_type == 'linear':\n",
    "            betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        elif schedule_type == 'cosine':\n",
    "            s = 0.008\n",
    "            t = torch.linspace(0, timesteps, timesteps + 1)\n",
    "            alphas_cumprod = torch.cos(((t / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
    "            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "            betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "            betas = torch.clip(betas, 0.0001, 0.9999)\n",
    "        elif schedule_type == 'gamma_rebalanced':\n",
    "            t = torch.linspace(0, 1, timesteps) ** gamma\n",
    "            betas = beta_start + (beta_end - beta_start) * t\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule: {schedule_type}\")\n",
    "        \n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        self.betas = betas.to(device)\n",
    "        self.alphas = alphas.to(device)\n",
    "        self.alphas_cumprod = alphas_cumprod.to(device)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).to(device)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod).to(device)\n",
    "    \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        sqrt_alpha = self.sqrt_alphas_cumprod[t][:, None, None, None]\n",
    "        sqrt_one_minus = self.sqrt_one_minus_alphas_cumprod[t][:, None, None, None]\n",
    "        return sqrt_alpha * x_start + sqrt_one_minus * noise, noise\n",
    "\n",
    "class FrequencyAwareLoss:\n",
    "    def __init__(self, freq_weight=0.1):\n",
    "        self.freq_weight = freq_weight\n",
    "    \n",
    "    def __call__(self, pred, target):\n",
    "        spatial_loss = F.mse_loss(pred, target)\n",
    "        pred_fft = torch.abs(torch.fft.fft2(pred))\n",
    "        target_fft = torch.abs(torch.fft.fft2(target))\n",
    "        freq_loss = F.mse_loss(pred_fft, target_fft)\n",
    "        total = spatial_loss + self.freq_weight * freq_loss\n",
    "        return total, {'loss': total.item(), 'spatial': spatial_loss.item(), 'freq': freq_loss.item()}\n",
    "\n",
    "class DDIMSampler:\n",
    "    def __init__(self, schedule, model, eta=0.0):\n",
    "        self.schedule = schedule\n",
    "        self.model = model\n",
    "        self.eta = eta\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape, num_steps=50, device='cuda', progress=True):\n",
    "        self.model.eval()\n",
    "        x = torch.randn(shape, device=device)\n",
    "        \n",
    "        step_size = self.schedule.timesteps // num_steps\n",
    "        timesteps = torch.arange(self.schedule.timesteps - 1, -1, -step_size, device=device)\n",
    "        \n",
    "        iterator = tqdm(timesteps, desc='DDIM') if progress else timesteps\n",
    "        \n",
    "        for i, t in enumerate(iterator):\n",
    "            t_batch = t.expand(shape[0])\n",
    "            noise_pred = self.model(x, t_batch)\n",
    "            \n",
    "            alpha_t = self.schedule.alphas_cumprod[t]\n",
    "            alpha_prev = self.schedule.alphas_cumprod[max(0, t - step_size)] if t > 0 else torch.tensor(1.0)\n",
    "            \n",
    "            x0_pred = (x - torch.sqrt(1 - alpha_t) * noise_pred) / torch.sqrt(alpha_t)\n",
    "            x0_pred = torch.clamp(x0_pred, -1, 1)\n",
    "            \n",
    "            sigma = self.eta * torch.sqrt((1 - alpha_prev) / (1 - alpha_t) * (1 - alpha_t / alpha_prev))\n",
    "            dir_xt = torch.sqrt(1 - alpha_prev - sigma**2) * noise_pred\n",
    "            noise = torch.randn_like(x) if self.eta > 0 and t > 0 else 0\n",
    "            \n",
    "            x = torch.sqrt(alpha_prev) * x0_pred + dir_xt + sigma * noise\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Diffusion utilities defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2935bd5",
   "metadata": {},
   "source": [
    "## 4. Create Synthetic Dataset (or load real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ee51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_brain_data(num_samples=500, size=128):\n",
    "    \"\"\"Create synthetic brain-MRI-like images for testing.\"\"\"\n",
    "    from scipy.ndimage import gaussian_filter\n",
    "    \n",
    "    images = []\n",
    "    for _ in tqdm(range(num_samples), desc=\"Generating synthetic data\"):\n",
    "        img = np.zeros((size, size), dtype=np.float32)\n",
    "        \n",
    "        # Create elliptical brain shape\n",
    "        y, x = np.ogrid[:size, :size]\n",
    "        cx, cy = size // 2, size // 2\n",
    "        rx, ry = size // 2.5, size // 2.5\n",
    "        \n",
    "        outer = ((x - cx) / rx) ** 2 + ((y - cy) / ry) ** 2 <= 1\n",
    "        inner = ((x - cx) / (rx * 0.9)) ** 2 + ((y - cy) / (ry * 0.9)) ** 2 <= 1\n",
    "        \n",
    "        img[outer] = 0.3 + np.random.uniform(-0.1, 0.1)\n",
    "        img[inner] = 0.6 + np.random.uniform(-0.1, 0.1)\n",
    "        \n",
    "        # Add ventricles\n",
    "        for _ in range(2):\n",
    "            vx = cx + np.random.randint(-size//6, size//6)\n",
    "            vy = cy + np.random.randint(-size//8, size//8)\n",
    "            vrx, vry = np.random.randint(5, 15), np.random.randint(5, 15)\n",
    "            vent = ((x - vx) / vrx) ** 2 + ((y - vy) / vry) ** 2 <= 1\n",
    "            img[vent & inner] = 0.2\n",
    "        \n",
    "        # Add noise and blur\n",
    "        img += np.random.normal(0, 0.05, (size, size))\n",
    "        img = np.clip(img, 0, 1)\n",
    "        img = gaussian_filter(img, sigma=1)\n",
    "        \n",
    "        images.append(img)\n",
    "    \n",
    "    images = np.array(images)[:, None, :, :]  # [N, 1, H, W]\n",
    "    return torch.from_numpy(images).float() * 2 - 1  # Normalize to [-1, 1]\n",
    "\n",
    "# Create dataset\n",
    "print(\"Creating synthetic dataset...\")\n",
    "train_data = create_synthetic_brain_data(num_samples=400, size=CONFIG['data']['resolution'])\n",
    "val_data = create_synthetic_brain_data(num_samples=100, size=CONFIG['data']['resolution'])\n",
    "\n",
    "print(f\"Train data: {train_data.shape}\")\n",
    "print(f\"Val data: {val_data.shape}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow((train_data[i, 0].numpy() + 1) / 2, cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Synthetic Brain MRI Slices')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b1ea4",
   "metadata": {},
   "source": [
    "## 5. Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f579c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VAE\n",
    "vae = VAE(\n",
    "    in_channels=CONFIG['vae']['in_channels'],\n",
    "    latent_channels=CONFIG['vae']['latent_channels'],\n",
    "    hidden_dims=CONFIG['vae']['hidden_dims']\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in vae.parameters())\n",
    "print(f\"VAE parameters: {num_params:,}\")\n",
    "print(f\"Downsample factor: {vae.downsample_factor}x\")\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(train_data),\n",
    "    batch_size=CONFIG['vae']['batch_size'],\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(TensorDataset(val_data), batch_size=CONFIG['vae']['batch_size'])\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(vae.parameters(), lr=CONFIG['vae']['learning_rate'])\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['vae']['epochs'])\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE\n",
    "print(\"Training VAE...\")\n",
    "vae_history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(CONFIG['vae']['epochs']):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for (batch,) in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['vae']['epochs']}\", leave=False):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            recon, mean, logvar = vae(batch)\n",
    "            loss, _ = vae.loss_function(batch, recon, mean, logvar, CONFIG['vae']['kl_weight'])\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    vae.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            recon, mean, logvar = vae(batch, sample_posterior=False)\n",
    "            loss, _ = vae.loss_function(batch, recon, mean, logvar, CONFIG['vae']['kl_weight'])\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    vae_history['train_loss'].append(train_loss)\n",
    "    vae_history['val_loss'].append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}: train={train_loss:.4f}, val={val_loss:.4f}\")\n",
    "\n",
    "print(\"VAE training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VAE reconstructions\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch = val_data[:8].to(device)\n",
    "    recon, _, _ = vae(test_batch, sample_posterior=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow((test_batch[i, 0].cpu().numpy() + 1) / 2, cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title('Original' if i == 0 else '')\n",
    "    \n",
    "    axes[1, i].imshow((recon[i, 0].cpu().numpy() + 1) / 2, cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title('Recon' if i == 0 else '')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(vae_history['train_loss'], label='Train')\n",
    "plt.plot(vae_history['val_loss'], label='Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('VAE Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f23a72",
   "metadata": {},
   "source": [
    "## 6. Train Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset to latents\n",
    "print(\"Encoding dataset to latents...\")\n",
    "vae.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_latents = []\n",
    "    for (batch,) in tqdm(train_loader, desc=\"Encoding train\"):\n",
    "        z = vae.get_latent(batch.to(device), sample=False)\n",
    "        train_latents.append(z.cpu())\n",
    "    train_latents = torch.cat(train_latents, dim=0)\n",
    "    \n",
    "    val_latents = []\n",
    "    for (batch,) in val_loader:\n",
    "        z = vae.get_latent(batch.to(device), sample=False)\n",
    "        val_latents.append(z.cpu())\n",
    "    val_latents = torch.cat(val_latents, dim=0)\n",
    "\n",
    "print(f\"Train latents: {train_latents.shape}\")\n",
    "print(f\"Val latents: {val_latents.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bbc74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNet and diffusion components\n",
    "unet = UNet(\n",
    "    in_ch=CONFIG['unet']['in_channels'],\n",
    "    out_ch=CONFIG['unet']['out_channels'],\n",
    "    model_ch=CONFIG['unet']['model_channels'],\n",
    "    ch_mult=CONFIG['unet']['channel_mult'],\n",
    "    num_res=CONFIG['unet']['num_res_blocks'],\n",
    "    attn_res=CONFIG['unet']['attention_resolutions'],\n",
    "    dropout=CONFIG['unet']['dropout']\n",
    ").to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in unet.parameters())\n",
    "print(f\"UNet parameters: {num_params:,}\")\n",
    "\n",
    "# Noise schedule\n",
    "schedule = NoiseSchedule(\n",
    "    schedule_type=CONFIG['diffusion']['beta_schedule'],\n",
    "    timesteps=CONFIG['diffusion']['timesteps'],\n",
    "    beta_start=CONFIG['diffusion']['beta_start'],\n",
    "    beta_end=CONFIG['diffusion']['beta_end'],\n",
    "    gamma=CONFIG['diffusion']['gamma'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = FrequencyAwareLoss(freq_weight=CONFIG['diffusion']['freq_loss_weight'])\n",
    "optimizer = optim.AdamW(unet.parameters(), lr=CONFIG['diffusion']['learning_rate'])\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Dataloader\n",
    "latent_loader = DataLoader(\n",
    "    TensorDataset(train_latents),\n",
    "    batch_size=CONFIG['diffusion']['batch_size'],\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868fb681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train diffusion\n",
    "print(\"Training diffusion model...\")\n",
    "diff_history = {'loss': []}\n",
    "global_step = 0\n",
    "total_steps = CONFIG['diffusion']['training_steps']\n",
    "\n",
    "latent_iter = iter(latent_loader)\n",
    "pbar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "for step in pbar:\n",
    "    try:\n",
    "        (z,) = next(latent_iter)\n",
    "    except StopIteration:\n",
    "        latent_iter = iter(latent_loader)\n",
    "        (z,) = next(latent_iter)\n",
    "    \n",
    "    z = z.to(device)\n",
    "    t = torch.randint(0, schedule.timesteps, (z.size(0),), device=device)\n",
    "    noise = torch.randn_like(z)\n",
    "    z_noisy, noise_target = schedule.q_sample(z, t, noise)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    with autocast():\n",
    "        noise_pred = unet(z_noisy, t)\n",
    "        loss, loss_dict = loss_fn(noise_pred, noise_target)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)\n",
    "    nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    diff_history['loss'].append(loss_dict['loss'])\n",
    "    pbar.set_postfix({'loss': f\"{loss_dict['loss']:.4f}\"})\n",
    "    global_step += 1\n",
    "\n",
    "print(\"Diffusion training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot diffusion loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(diff_history['loss'])\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Diffusion Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5732cc25",
   "metadata": {},
   "source": [
    "## 7. Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b49683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with DDIM\n",
    "print(\"Generating samples...\")\n",
    "sampler = DDIMSampler(schedule, unet, eta=CONFIG['sampling']['eta'])\n",
    "\n",
    "unet.eval()\n",
    "vae.eval()\n",
    "\n",
    "latent_size = CONFIG['data']['resolution'] // vae.downsample_factor\n",
    "sample_shape = (16, CONFIG['unet']['in_channels'], latent_size, latent_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_samples = sampler.sample(sample_shape, num_steps=CONFIG['sampling']['ddim_steps'], device=device)\n",
    "    generated = vae.decode(z_samples)\n",
    "    generated = torch.clamp((generated + 1) / 2, 0, 1)\n",
    "\n",
    "print(f\"Generated {len(generated)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generated samples\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(generated[i, 0].cpu().numpy(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Generated Brain MRI Slices (DDIM 50 steps)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532fb28",
   "metadata": {},
   "source": [
    "## 8. Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2637d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SSIM and PSNR for VAE reconstructions\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "vae.eval()\n",
    "ssim_scores = []\n",
    "psnr_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (batch,) in val_loader:\n",
    "        batch = batch.to(device)\n",
    "        recon, _, _ = vae(batch, sample_posterior=False)\n",
    "        \n",
    "        # Convert to [0, 1]\n",
    "        batch_01 = (batch.cpu().numpy() + 1) / 2\n",
    "        recon_01 = np.clip((recon.cpu().numpy() + 1) / 2, 0, 1)\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            s = ssim(batch_01[i, 0], recon_01[i, 0], data_range=1.0)\n",
    "            p = psnr(batch_01[i, 0], recon_01[i, 0], data_range=1.0)\n",
    "            ssim_scores.append(s)\n",
    "            psnr_scores.append(p)\n",
    "\n",
    "print(f\"\\nVAE Reconstruction Metrics:\")\n",
    "print(f\"  SSIM: {np.mean(ssim_scores):.4f} ± {np.std(ssim_scores):.4f}\")\n",
    "print(f\"  PSNR: {np.mean(psnr_scores):.2f} ± {np.std(psnr_scores):.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9734179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction error heatmap\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    sample = val_data[0:1].to(device)\n",
    "    recon, _, _ = vae(sample, sample_posterior=False)\n",
    "\n",
    "error = torch.abs(sample - recon).squeeze().cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow((sample[0, 0].cpu().numpy() + 1) / 2, cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow((recon[0, 0].cpu().numpy() + 1) / 2, cmap='gray')\n",
    "axes[1].set_title('Reconstructed')\n",
    "axes[1].axis('off')\n",
    "\n",
    "im = axes[2].imshow(error, cmap='hot')\n",
    "axes[2].set_title('Error Heatmap')\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(im, ax=axes[2], fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545f2e7",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SBLDM Training Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nVAE:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in vae.parameters()):,}\")\n",
    "print(f\"  Final train loss: {vae_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final val loss: {vae_history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  SSIM: {np.mean(ssim_scores):.4f}\")\n",
    "print(f\"  PSNR: {np.mean(psnr_scores):.2f} dB\")\n",
    "\n",
    "print(f\"\\nDiffusion UNet:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in unet.parameters()):,}\")\n",
    "print(f\"  Training steps: {global_step}\")\n",
    "print(f\"  Final loss: {diff_history['loss'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nGeneration:\")\n",
    "print(f\"  DDIM steps: {CONFIG['sampling']['ddim_steps']}\")\n",
    "print(f\"  Samples generated: {len(generated)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"- Train longer (100+ VAE epochs, 50K+ diffusion steps)\")\n",
    "print(\"- Use real BraTS/medical imaging data\")\n",
    "print(\"- Compute FID with more samples\")\n",
    "print(\"- Experiment with gamma-rebalanced schedule\")\n",
    "print(\"- Test adaptive sampling\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08780f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models (optional)\n",
    "# torch.save(vae.state_dict(), 'vae_weights.pt')\n",
    "# torch.save(unet.state_dict(), 'unet_weights.pt')\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
